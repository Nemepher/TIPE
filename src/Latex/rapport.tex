\documentclass{article}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{subcaption}
\usepackage[toc]{appendix}
\usepackage[nottoc]{tocbibind}
\pagestyle{headings}

\pgfplotsset{width=5cm,compat=1.16}

\title{Détection et identification d'arbre à partir d'imagerie satellite/aerienne}
\author{Augustin Albert}

\begin{document}

\maketitle
\tableofcontents

\section*{Introduction}
	\subsection*{Position du problème}
		-pourquoi vouloir faire ça, utilité/contexte	
		-1 pb extraction de données : differentes méthodes qui requièrent plus ou moins de materiel/images de qualité
		-2 pb traitement des données

	\subsection*{\'{E}tat actuel de la recherche}
		-voir papiers 

	\subsection*{Objectifs du TIPE}		
		-limitation à des images aériennes: pourquoi(moins couteux, accessible sur internet, differents modes d'aquisitions,  enjeux/difficultés
		-d'une part à concevoir ... pour detecter les a
		-d'une autre part à l'utiliser pour contruire une base de donné permetant identification ulterierure sur la base du machin learning
		-application au site du parc regional...
		
\section{Détection des houppiers}
	Intro: Méthode naive ( les preentations) de detection des zones plus lumineuses. La luminosité des arbres peut beaucoup varier sur une meme image (à moins d'avoir des images de haute qualité "prise en une seule fois" (ex papier). Une solution = Détection de blobs. 2 problemes : -différence de luminosité et différence d'échelle. Citer algo pour. de pixels plus lumineux que leur voisins correspondants aux houppiers

	\subsection{Laplacien du gaussien et approche multi échelle}
		Afin de prendre en compte des houppiers dont le diamètre peut varier considérablement entre les différentes espèces mais aussi au sein d'une même espèce, une approche multi-échelle est nécessaire. Nous utiliserons la théorie échelle-temps dévellopée par (\ref Lindbergh). Une pyramide d'echelle est réalisée en lissant successivement l'image originelle avec un filtre gaussien de paramètre $\sigma$ : le paramètre d'échelle. L'image originelle est convolé autant de fois que nécessaire par la fonction gaussiene suivante, le paramètre $\sigma$ étant multiplié par un ratio fixé à !!!!! important !!!! à chaqe étape : 
	
	\[G_{\sigma}:=\frac{1}{2\pi\sigma^{2}}\exp(-\frac{x^{2}+y^{2}}{2\sigma^{2}})\]

	On applique alors un opérateur laplacien normalisé aux images résultante afin d'obtenir la pyramide d'échelle du laplacien du Gaussien (LoG). L'image obtenue à chaque niveau est alors la convolution de l'image originelle par la fonction Log :


	\[{LoG}_{\sigma}:=-\frac{1}{\pi\sigma^{4}}(1-\frac{x^{2}+y^{2}}{2\sigma^{2}})\exp(-\frac{x^{2}+y^{2}}{2\sigma^{2}})\] 

	En pratique, le LoG est approximé par la différence du gaussien (DoG) obtenue en réalisant la difference des images floutées entre chaque niveau de la pyramide d'échelle.  

	Cette pyramide d'échelle du LoG permet d'extraire des caractéristiques indépendament de leur échelle en exploitant la réponse de l'opérateur LoG appliqué à un signal échelon. (figure 5). Lorsque le rayon caractéristique du blob $r$ varie, le minimum (maximum en valeur absolue) du LoG est atteint au centre du blob. Lorsque le paramètre $\sigma$ varie, la réponse au centre est minimale lorsque $r$ est relié à $\sigma$ par la relation $\sigma=\frac{r}{\sqrt{2}}$. La réponse du LoG non normalisé s'attenuant lorsque $\sigma$ augmente, l'opérateur à été multiplié par $\sigma^{2}$ pour que la réponse soit indépendante de l'échelle.

	La détection des blobs se ramène ainsi à la recherche d'un minimum local relativement à l'espace et global relativement à l'échelle pour identifier à la fois les centres des houppier et la taille caractéristique de leur rayon. 
	
	-figure 1 réponse à un echellon 
	-figure 2 un blob mais différents paramètres de largeur et de parametre !!

\begin{figure}
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\caption{1}
	\end{subfigure}
	\begin{subfigure}{.5\textwidth}
		\centering
		\caption{2}
	\end{subfigure}
	\begin{subfigure}{.5\textwidth}
		\centering
		\caption{3}
	\end{subfigure}
	\caption{test2}
\end{figure}


\begin{figure}
	\centering
	\begin{tikzpicture}[scale=.7]
		\begin{scope}[yslant=0.5, xslant=-1]
			\fill[step=5mm, blue] (2,1.5) rectangle (3.5,3);
			\draw[step=5mm, black] (1,1) grid (4,4);
			\draw[step=5mm, thick, black] (1,1) rectangle (4,4);
		\end{scope}
		\begin{scope}[yshift=50, yslant=0.5, xslant=-1]
			\fill[step=5mm, white] (1,1) rectangle (4,4);
			\fill[step=5mm, blue] (2,1.5) rectangle (3.5,3);
			\fill[step=5mm, red] (2.5,2) rectangle (3,2.5);
			\draw[step=5mm, black] (1,1) grid (4,4);
			\draw[step=5mm, thick, black] (1,1) rectangle (4,4);
		\end{scope}
		\begin{scope}[yshift=100, yslant=0.5, xslant=-1]
			\fill[step=5mm, white] (1,1) rectangle (4,4);
			\fill[step=5mm, blue] (2,1.5) rectangle (3.5,3);
			\draw[step=5mm, black] (1,1) grid (4,4);
			\draw[step=5mm, thick, black] (1,1) rectangle (4,4);
		\end{scope}
		\end{tikzpicture}
	\caption{test un deux}
\end{figure}

	\subsection{Mise en place de l'algorithme}

	L'algorithme envisagé à été implémenté à l'aide du langage Python et utilise la bibliothèque Numpy afin d'accélérer le traitement des tableaux. Il comporte trois étapes. 
	
	- L'image à traité est convertie en nuance de gris et éventuellement inversée afin que le fond soit plus clair que les houppiers. (une intervention humaine est nécessaire). 
	
	- La pyramide d'échelle du DoG est généré. Les noyaux de Gauss et la convolution ont été implémenté en exploitant la séparablilité du filtre de Gauss pour accélérer le calcul, mais en définitive, la convolution fourni par le module scipy à été utilisé. Les résultats sont stockés dans un tableau Numpy 3D.  

	- Les minimums sont calculés en parcourant le tableau par profondeur croissante afin de ne conserver que les houppiers de rayon maximum lors d'éventuels chevauchements.    
	
	3 paramètres permettent d'ajuster les résultats. La sélection des paramètres dépend fortement du (très heuristique comme méthode, nécessite des essais => la méthodes n'est pas completement automatique. (dépend de la taille caractéristique des arbres, de l'echelle choisie) donner les paramètre pour l'echelle et tout.

		facteur limitant i
		choix final des paramètres : + image utilisées (taille des arbres sur le terrain et échelle de l'image) 

	\subsection{\'{E}valuation des résultats}
		-évalutation de la complexité 
		-propres résultats
		faire tableau 3 colonnes pour les trois images différentes
		% repéré 
		-comparaison avec les résultats des papiers 

\section{Identification des espèces}

	Choix du machin learning comme dans de nombreux papiers (meme si pas les memes donnees de base) Propre implementation dévellopé mais aps complet et ne permet pas à ce jour de traiter efficacement des images (maque le truc de convolution et reste lent en comparaison des systemes professionels) Le choix s'est donc porté sur Tensorflow.  

	\subsection{Méthodologie de construction d'une base de donné fiable}
		Il aurait été fastidieux d'étiqetter à la main un nombre important d'images, d'autant plus qu'elles sont de basses réolution. Afin   
		- géoportail (verif autorisation... cé) et extration sur des zones ou la couverture d'espèce est uniforme : res --- images triées en 2
	
	\subsection{entrainement et quelle type de modèle }
		-citer papier , à la main car ici le cas est plus simple mais l'on pourrais généraliser. (difference avec eux)

\section{Prolongements envisagables}

	Différents prolongement serait envisagables: 
	-Sensible aux ... séparer préalablement et éventuellement grossiemerement les zones forêstières des zones d'habitation ou  industrielle. Même une route bétonné peut éventuellement altérer les réusltats. batiement/contruction quo fausserait les resultat. 
	De plus obtient qu'un cercle autour des arbres
	-une methode watershed segmentation avec marqueurs que l'on à trouvé pourrait etre envisagble pour delinéer parfaitement les arbres (voir papier) 

\nocite{NatesanResNet} %à supprimer après!! 
\bibliographystyle{alpha}
\bibliography{references}

\begin{appendix}

	\section{Compléments}
		Lien avec filtre basse fréquence (downsampling, pourqoi?)
		le filtre de gauss retire les hautes fréquences donc permet d' augmenter la fréquence d'échantillonage 'théorme de shannon sans qu'il y ait d'artefacts. 
	
	\section{Résultats}	
		-résultats intermédiare (pyramide de gauss)
		-les deux 
		-douglas seul 
		-feuillus seul
		-echantillon banque fourni pour feuillus
		-echantillon banque fourni pour douglas 
		-echantillon aléatoire parmis des images non deja vus
	
	\section{algorithmes}
\end{appendix}


a faire : réaliser les tableaux de comparaison, mettre en forme le code 
ecrire les preuves 

\end{document}


